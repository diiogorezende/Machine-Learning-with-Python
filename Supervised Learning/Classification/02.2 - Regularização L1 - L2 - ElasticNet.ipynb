{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPj69jxHsWyJN+nXpJj2BRT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Regularizações\n","Aplicamos técnicas de regularização quando estamos preocupados não apenas obter o valor ótimo e minimizar a função de custo. Ela serve para que seja possível controlar a complexidade do nosso modelo, penalizando coeficientes pouco relevantes para o problema. Isso pode reduzir a variância do modelo, tornando-o mais robusto a ruídos e outliers, e também o torna mais generalizável.\n","\n","## Regularização L1\n","Adiciona a soma dos valores absolutos dos coeficientes como uma penalidade. Com isso, tende a forçar alguns coeficientes a serem zero, o que pode deixar o modelo mais interpretável ao realizar um tipo de seleção de variáveis.\n","\n","Usamos ela quando queremos realizar um tipo de seleção de variáveis, simplificar o nosso modelo, quando possuímos datasets muito esparsos, e quando há alta dimensionalidade de variáveis.\n","\n","**Vantagens:**\n","- Tende a reduzir os coeficientes a zero, fazendo com que fiquem apenas variáveis relevantes\n","- É de fácil interpretação\n","\n","**Desvantagens:**\n","- Quando existem muitas variáveis correlacionadas, a L1 pode realizar escolhas aleatoriamente, fazendo com que algumas variáveis fiquem de fora, levando a instabilidade dos coeficientes.\n","- Quando possuímos muitas variáveis relevantes, a L1 pode não ser tão performática\n","\n","Função de perda com a regularização L1:\n","$$\n","J(\\theta) = \\text{MSE} + \\lambda \\sum_{i=1}^{n} |\\theta_i|\n","$$\n","\n","## Regularização L2\n","Adiciona a soma dos quadrados dos coeficientes como uma penalidade. Ao contrário da L1, ela não zera os coeficientes, mas pode reduzi-los bastante. Dessa forma, a regularização L2 ajuda a evitar que os coeficientes se tornem muito grandes.\n","\n","**Vantagens:**\n","- Mantém as variáveis no modelo, apenas reduzindo a magnetude dos coeficientes. Isso se faz útil quando as variáveis possuem alguma relevância.\n","- Distribui pesos de forma mais equilibrada quando existem muitas correlações.\n","\n","**Desvantagens:**\n","- Não realiza nenhum tipo de seleção de características.\n","- Interpretação um pouco mais difícil, pelo fato de não eliminar características.\n","\n","Usamos quando todas as variáveis possuem alguma importância, pois ela diminui o peso dos coeficientes sem força-los a zerar. Também é útil para datasets pequenos, pois a L2 ajuda a controlar o overfitting sem remover variáveis.\n","\n","\n","A função de perda com a regularização L2:\n","$$\n","J(\\theta) = \\text{MSE} + \\lambda \\sum_{i=1}^{n} \\theta_i^2\n","$$\n","\n","## Regularização ElasticNet\n","Ela combina as penalidades da regularização L1 e L2. Ela adiciona uma soma ponderada dessas duas penalidades. Se mostra útil quando há correlações entre as variáveis preditoras e desejamos aproveitar tanto a seleção de variáveis, quanto a suavização dos coeficientes.\n","\n","**Vantagens:**\n","- Combina as regularizações L1 e L2, permitindo escolher qual peso cada uma delas terá.\n","- Permite maior balanceamento entre e L1 e a L2, de forma a ajustarmos de acordo com o problema.\n","\n","**Desvantagens:**\n","- Temos dois parametros para ajustar, então acaba sendo mais complexo.\n","- Custo computacional maior.\n","\n","Usamos quando existem muitas variáveis correlacionadas e variáveis irrelevante, e desejamos combinar sparsidade e estabilidade, pois podemos ponderar a quantidade de L1 e L2.\n","\n","A função de perda com a regularização ElasticNet:\n","$$\n","J(\\theta) = \\text{MSE} + \\lambda_1 \\sum_{i=1}^{n} |\\theta_i| + \\lambda_2 \\sum_{i=1}^{n} \\theta_i^2\n","$$"],"metadata":{"id":"O-O8OaTTsSNC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxsuGnB7sHQX"},"outputs":[],"source":[]}]}