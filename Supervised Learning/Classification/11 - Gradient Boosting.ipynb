{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN89Ck8ChOmGsqGdvNvEkXB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# __Gradient Boosting para Regressão__\n","É um método de ensemble que cria um modelo forte a partir da combinação de vários modelos fracos.\n","\n","A ideia do _Gradient Boosting_ é construir uma série de árvores de decisão, onde cada árvore seguinte tenta corrigir os erros da árvore anterior. Para isso, se baseia no cálculo do erro residual - que é a diferença entre valor real e os valores previstos. Essa correção é feita usando a técnica do gradiente da função de perda.\n","\n","## __Funcionamento__\n","- 1) __Inicialização:__ Inicia com uma previsão inicial, que geralmente é a média dos valores da variável alvo no conjunto de treino.\n","- 2) __Cálculo dos Resíduos:__ Pra cada observação, calcula o erro - que é a diferença entre o valor real e o valor previsto.\n","- 3) __Correção:__ A árvore seguinte é criada ajustando-se ao erro da árvore anterior. Ou seja, ela aprende a partir do erro da sua antecessora.\n","- 4) __Atualização das Previsões:__ A previsão final é ajustada ao se somar uma fração da nova árvore aos valores atuais. Essa fração é controlada pelo __learning rate__, que define o peso com que a nova árvore contribui para a correção dos erros anteriores.\n","- 5) __Iterações:__ Todo esse processo é repetido diversas vezes, construindo um conjunto de árvores, onde cada uma tenta corrigir o erro da árvore anterior.\n","\n","## __Processamento e Pré-processamento__\n","- 1) __Pré-processamento:__ Podemos aplicar escalonamento nos dados numéricos para melhorar o desempenho. Geralmente, variáveis categóricas são transformadas em dummies.\n","- 2) __Função de Perda:__ A função de perda mais comum é o MSE, que é uma função sensível a valores extremos e penaliza grandes erros.\n","- 3) __Hiperparâmetros:__ Ajustar o número de estimadores, learning rate, profundidade máxima das árvores.\n","\n","## __Casos de Uso__\n","Usamos ele quando precisamos encontrar padrões complexos e não-lineares, como mas sem se restringir a:\n","- Valores do mercado imobiliário;\n","- Demanda de energia;\n","- Séries temporais.\n","\n","## __Vantagens__\n","Robusto contra outliers e tem bom desempenho quando alimentado com dados complexos.\n","\n","## __Desvantagens__\n","Pode acabar em overfitting e demorar para completar seu treinamento."],"metadata":{"id":"RXn50t0MlIpN"}},{"cell_type":"markdown","source":["# __Gradient Boosting para Classificação__\n","Para problemas de classificação, o funcionamento é similar a quando lidamos com problemas de regressão. Porém, existe uma abordagem diferente na função de perda, que é mais específica para tarefas de classificação.\n","\n","É criada uma sequência de modelos fracos, onde cada modelo seguinte irá aprender com os erros do modelo anterior. Para as tarefas de classificação, o algoritmo usa a probabilidade de uma observação pertencer a uma determinada classe, ajustando o modelo para melhorar a precisão.\n","\n","## __Funcionamento__\n","- 1) __Inicialização:__ O modelo é iniciado com uma previsão básica - que pode ser a classe mais comum do conjunto de treino.\n","- 2) __Cálculo dos Resíduos:__ O erro é calculado com base na função de perda, como o log loss, que mede a diferença entre a probabilidade prevista e a probabilidade real de cada classe.\n","- 3) __Correção:__ Em cada nova iteração, uma nova árvore é ajustada para prever os erros. Essa árvore é treinada para melhorar a probabilidade das classes verdadeiras.\n","- 4) __Atualização das Previsões:__ Similar ao que falamos em regressão, a previsão final é ajustada ao se somar uma fração da nova árvore aos valores atuais. Essa fração é controlada pelo learning rate, que define o peso com que a nova árvore contribui para a correção dos erros anteriores.\n","- 5) __Iterações:__ Como na regressão, este processo é repetido, de forma a ir ajustando o modelo para melhorar a precisão.\n","\n","## __Processamento e Pré-processamento__\n","- 1) __Pré-processamento:__ Podemos aplicar escalonamento e normalização nos dados numéricos para melhorar o desempenho. Geralmente, variáveis categóricas são transformadas em dummies.\n","- 2) __Função de Perda:__ Na classificação binária, a função de perda mais comum é a __log loss__, que é mais sensível a erros de classificação. Para classificação multiclasse, podemos usar uma versão generalizada da log loss.\n","- 3) __Hiperparâmetros:__ Podemos trabalhar em cima do número de estimadores, learning rate, profundidade das árvores, etc.\n","\n","## __Casos de Uso__\n","Podemos aplicar em situações como, mas sem se restringir a:\n","- Detecção de fraude;\n","- Diagnóstico médico;\n","- Classificação de imagens.\n","\n","## __Vantagens__\n","Possui uma alta precisão e bom desempenho em dados não-lineares e complexos.\n","\n","## __Desvantagens__\n","Pode acabar em overfitting se não for bem trabalhado. Possui um treinamento mais demorado se comparado a outros algoritmos, como Random Forest."],"metadata":{"id":"fAAbRbbwZX12"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAoWfdrls2Hm"},"outputs":[],"source":[]}]}